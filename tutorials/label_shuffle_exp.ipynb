{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Shuffle Experiment for Progressive Learning\n",
    "\n",
    "Most state of the art algorithms are unable to transfer knowledge forward, and none are able to transfer knowledge backward, both key capabilities in progressive learning. This inability to transfer has been identified as one of the key obstacles limiting the capabilities of artificial intelligence. \n",
    "\n",
    "Representation ensembling algorithms sequentially learn a representation for each task, and ensemble both old and new representations for all future decisions. Two algorithms for progressive learning is proposed. Lifelong Learning Forest `L2F` uses decision forests as the transformers, specifically a variant of decision forests called ‘Uncertainty Forest’ `UF`. To obtain consistent estimates of the posteriors, each tree is ‘honest’, meaning that it uses each data point for either learning the transformer or voter, but not both. Lifelong Learning Network `L2N` uses deep networks as the transformers. \n",
    "\n",
    "### Import necessary packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from math import log2, ceil \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models from proglearn\n",
    "Append the path for where your proglearn to sys\n",
    "\n",
    "Important: Change internal calculation of k in voter.py to `16 * int(np.log2(len(X)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../proglearn/\")\n",
    "from forest import LifelongClassificationForest \n",
    "from network import LifelongClassificationNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR100 data \n",
    "First we load the CIFAR100 data from keras.datasets and store it in `data_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "data_x = np.concatenate([X_train, X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We define the functions for training the model\n",
    "\n",
    "`cross_val_data`: splits the data and class labels to training and test \n",
    "\n",
    "`run_parallel_exp`: is a wrapper function for LF_experiment, and configures GPUs for training\n",
    "\n",
    "`LF_experiment`: Function that creates the progressive learner model and trains it. In `file_to_save`, specify the directory for the pickle files. \n",
    "\n",
    "For the `DNN`, `network` stores the keras sequential model for the neural network. The architecture consists of 5 convolutional layers of kernel size 3x3, with relu activation. Each activation is followed by a batch normalization layer. After the convolutional layers, there are 3 fully connected layers, and the last layer has a softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_data(data_x, data_y, num_points_per_task, total_task=10, shift=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]\n",
    "    \n",
    "    batch_per_task=5000//num_points_per_task\n",
    "    sample_per_class = num_points_per_task//total_task\n",
    "\n",
    "    for task in range(total_task):\n",
    "        for batch in range(batch_per_task):\n",
    "            for class_no in range(task*10,(task+1)*10,1):\n",
    "                indx = np.roll(idx[class_no],(shift-1)*100)\n",
    "                \n",
    "                if batch==0 and class_no==0 and task==0:\n",
    "                    train_x = x[indx[batch*sample_per_class:(batch+1)*sample_per_class]]\n",
    "                    test_x = x[indx[batch*total_task+num_points_per_task:(batch+1)*total_task+num_points_per_task]]\n",
    "                    train_y = np.random.randint(low = 0, high = total_task, size = sample_per_class)\n",
    "                    test_y = np.random.randint(low = 0, high = total_task, size = total_task)\n",
    "                else:\n",
    "                    train_x = np.concatenate((train_x, x[indx[batch*sample_per_class:(batch+1)*sample_per_class]]), axis=0)\n",
    "                    test_x = np.concatenate((test_x, x[indx[batch*total_task+num_points_per_task:(batch+1)*total_task+num_points_per_task]]), axis=0)\n",
    "                    if task == 0:\n",
    "                        train_y = np.concatenate((train_y, y[indx[batch*sample_per_class:(batch+1)*sample_per_class]]), axis=0)\n",
    "                        test_y = np.concatenate((test_y, y[indx[batch*total_task+num_points_per_task:(batch+1)*total_task+num_points_per_task]]), axis=0)\n",
    "                    else:\n",
    "                        train_y = np.concatenate((train_y, np.random.randint(low = 0, high = total_task, size = sample_per_class)), axis=0)\n",
    "                        test_y = np.concatenate((test_y, np.random.randint(low = 0, high = total_task, size = total_task)), axis = 0)\n",
    "                \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_exp(data_x, data_y, n_trees, model, num_points_per_task, slot=0, shift=1):\n",
    "    train_x, train_y, test_x, test_y = cross_val_data(data_x, data_y, num_points_per_task, shift=shift)\n",
    "    \n",
    "    if model == \"dnn\":\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=config)\n",
    "        with tf.device('/gpu:'+str(shift % 4)):\n",
    "            LF_experiment(train_x, train_y, test_x, test_y, n_trees, shift, slot, model, num_points_per_task, acorn=12345)\n",
    "    else:\n",
    "        LF_experiment(train_x, train_y, test_x, test_y, n_trees, shift, slot, model, num_points_per_task, acorn=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_experiment(train_x, train_y, test_x, test_y, ntrees, shift, slot, model, num_points_per_task, acorn=None):\n",
    "    #Initialize the dataframe for pickle files to store results   \n",
    "    df = pd.DataFrame()\n",
    "    shifts = []\n",
    "    tasks = []\n",
    "    base_tasks = []\n",
    "    accuracies_across_tasks = []\n",
    "    train_times_across_tasks = []\n",
    "    inference_times_across_tasks = []\n",
    "    \n",
    "    #Initialize the progressive_learner model \n",
    "    progressive_learner = None\n",
    "    \n",
    "    if model == \"dnn\":\n",
    "        network = keras.Sequential()\n",
    "        network.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=np.shape(train_x)[1:]))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=32, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=64, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Conv2D(filters=254, kernel_size=(3, 3), strides = 2, padding = \"same\", activation='relu'))\n",
    "\n",
    "        network.add(layers.Flatten())\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation='relu'))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(units=10, activation = 'softmax'))\n",
    "        \n",
    "        progressive_learner = LifelongClassificationNetwork(network=network)\n",
    "        \n",
    "    elif model == \"uf\":\n",
    "        progressive_learner = LifelongClassificationForest(n_estimators=ntrees)\n",
    "\n",
    "    for task_ii in range(10):\n",
    "        print(\"Starting Task {} For Fold {} For Slot {}\".format(task_ii, shift, slot))\n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "\n",
    "        train_start_time = time.time()\n",
    "        progressive_learner.add_task(\n",
    "            X = train_x[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task], \n",
    "            y = train_y[task_ii*5000+slot*num_points_per_task:task_ii*5000+(slot+1)*num_points_per_task]\n",
    "            )\n",
    "        train_end_time = time.time()\n",
    "        \n",
    "        inference_start_time = time.time()\n",
    "        llf_task=progressive_learner.predict(\n",
    "            test_x[:1000], task_id=0\n",
    "            )\n",
    "        inference_end_time = time.time()\n",
    "        acc = np.mean(\n",
    "                    llf_task == test_y[:1000]\n",
    "                    )\n",
    "        accuracies_across_tasks.append(acc)\n",
    "        shifts.append(shift)\n",
    "        train_times_across_tasks.append(train_end_time - train_start_time)\n",
    "        inference_times_across_tasks.append(inference_end_time - inference_start_time)\n",
    "        \n",
    "        print(\"Accuracy Across Tasks: {}\".format(accuracies_across_tasks))\n",
    "        print(\"Train Times Across Tasks: {}\".format(train_times_across_tasks))\n",
    "        print(\"Inference Times Across Tasks: {}\".format(inference_times_across_tasks))\n",
    "            \n",
    "    df['data_fold'] = shifts\n",
    "    df['task'] = range(1, 11)\n",
    "    df['task_1_accuracy'] = accuracies_across_tasks\n",
    "    df['train_times'] = train_times_across_tasks\n",
    "    df['inference_times'] = inference_times_across_tasks\n",
    "\n",
    "    file_to_save = './result/'+model+str(ntrees)+'_'+str(shift)+'_'+str(slot)+'.pickle'\n",
    "    with open(file_to_save, 'wb') as f:\n",
    "        pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters for the model and run model\n",
    "For `model`, choose between dnn (deep neural network) and uf (uncertainty forest) \n",
    "\n",
    "`UF`: The default parameters is `n_trees=10`, `shift=7`.\n",
    "\n",
    "`DNN`: The default parameters is `shift=7`\n",
    "\n",
    "`n_trees` specifies the number of transformers in the uncertainty forest. Running the cell below will train the model, based on the parameter specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define which type of model here\n",
    "model = \"dnn\"\n",
    "num_points_per_task = 500\n",
    "\n",
    "#Data preprocessing\n",
    "if model == \"uf\":\n",
    "    data_x = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2] * data_x.shape[3]))\n",
    "data_y = np.concatenate([y_train, y_test])\n",
    "data_y = data_y[:, 0]\n",
    "\n",
    "slot_fold = range(int(5000 // num_points_per_task))\n",
    "\n",
    "if model == \"uf\":\n",
    "    shift_fold = range(1,7,1)\n",
    "    n_trees=[10]\n",
    "    iterable = product(n_trees,shift_fold,slot_fold)\n",
    "    Parallel(n_jobs=-2,verbose=1)(\n",
    "        delayed(run_parallel_exp)(\n",
    "                data_x, data_y, ntree, model, num_points_per_task, slot=slot, shift=shift\n",
    "                ) for ntree,shift,slot in iterable\n",
    "                )\n",
    "elif model == \"dnn\":\n",
    "    \n",
    "    for slot in slot_fold:\n",
    "        def perform_shift(shift):\n",
    "            return run_parallel_exp(data_x, data_y, 0, model, num_points_per_task, slot=slot, shift=shift)\n",
    "        \n",
    "        stage_1_shifts = range(1, 5)\n",
    "        with Pool(4) as p:\n",
    "            p.map(perform_shift, stage_1_shifts) \n",
    "            \n",
    "        stage_2_shifts = range(5, 7)\n",
    "        with Pool(4) as p:\n",
    "            p.map(perform_shift, stage_2_shifts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Functions for calculating the backward transfer efficiency. The backward transfer efficiency of $f_n$ for task $t$ given $n$ samples is $BTE^t(fn) := \\mathbb{E}[R^t (f_n)/R^t(f_n)]$ .\n",
    "\n",
    "We say an algorithm (positive) backward transfers for task t if and only if $BTE^t(fn) > 1$, or if $\\log BTEt(fn) > 0$. In other words, if $BTE^t(fn) > 1$, then the algorithm has used data associated with new tasks to improve performance on previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bte(err):\n",
    "    bte = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        bte.append(err[0] / err[i])\n",
    "    \n",
    "    return bte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "Run cell to generate plot of transfer efficiency\n",
    "\n",
    "In the `filename`, add the directory to where you output the pickle files earlier in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slots = 1\n",
    "shifts = 6\n",
    "alg_name = ['L2N','L2F']\n",
    "\n",
    "reps = slots*shifts\n",
    "btes = np.zeros((len(alg_name),10),dtype=float)\n",
    "\n",
    "for alg_no,alg in enumerate(alg_name):\n",
    "    bte_tmp = [[] for _ in range(reps)]\n",
    "\n",
    "    count = 0   \n",
    "    for slot in range(slots):\n",
    "        for shift in range(shifts):\n",
    "            if alg_no==0:\n",
    "                filename = './result/dnn0_'+str(shift+1)+'_'+str(slot)+'.pickle'\n",
    "            elif alg_no==1:\n",
    "                filename = './result/uf10_'+str(shift+1)+'_'+str(slot)+'.pickle'\n",
    "            else:\n",
    "                filename = 'benchmarking_algorthms_result/'+alg+'_'+str(shift+1)+'_'+str(slot)+'.pickle'\n",
    "\n",
    "            multitask_df = unpickle(filename)\n",
    "\n",
    "            err = []\n",
    "\n",
    "            for ii in range(10):\n",
    "                err.extend(\n",
    "                1 - np.array(\n",
    "                    multitask_df[multitask_df['task']==ii+1]['task_1_accuracy']\n",
    "                )\n",
    "                )\n",
    "            bte = get_bte(err)\n",
    "        \n",
    "            bte_tmp[count].extend(bte)\n",
    "            count+=1\n",
    "    \n",
    "    btes[alg_no] = np.mean(bte_tmp, axis = 0)\n",
    "    \n",
    "clr = [\"#00008B\", \"#e41a1c\", \"#a65628\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#CCCC00\"]\n",
    "c = sns.color_palette(clr, n_colors=len(clr))\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "\n",
    "for alg_no,alg in enumerate(alg_name):\n",
    "    if alg_no<2:\n",
    "        ax.plot(np.arange(1,11),btes[alg_no], c=c[alg_no], label=alg_name[alg_no], linewidth=3)\n",
    "    else:\n",
    "        ax.plot(np.arange(1,11),btes[alg_no], c=c[alg_no], label=alg_name[alg_no])\n",
    "\n",
    "ax.set_yticks([.9,.95, 1, 1.05,1.1,1.15,1.2])\n",
    "ax.set_xticks(np.arange(1,11))\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.set_xlabel('Number of tasks seen', fontsize=24)\n",
    "ax.set_ylabel('Transfer Efficiency', fontsize=24)\n",
    "ax.set_title(\"Label Shuffled CIFAR\", fontsize = 24)\n",
    "ax.hlines(1,1,10, colors='grey', linestyles='dashed',linewidth=1.5)\n",
    "right_side = ax.spines[\"right\"]\n",
    "right_side.set_visible(False)\n",
    "top_side = ax.spines[\"top\"]\n",
    "top_side.set_visible(False)\n",
    "plt.tight_layout()\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1,0.5), fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

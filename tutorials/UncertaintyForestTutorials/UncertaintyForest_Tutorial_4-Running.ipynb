{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Overview\n",
    "This set of five tutorials (installation, package setup, data setup, running, analyzing) will explain the UncertaintyForest class. After following the steps below, you should have the ability to run the code on your own machine and interpret the results.\n",
    "\n",
    "If you haven't seen it already, take a look at the first three parts of this set of tutorials called `UncertaintyForest_Tutorials_1-Installation`, `UncertaintyForest_Tutorial_2-Package-Setup`, and `UncertaintyForest_Tutorial_3-Data-Setup`\n",
    "\n",
    "# 4: Running\n",
    "## *Goal: Train the UncertaintyForest classifier on some training data and produce a metric of accuracy on some test data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: First, we'll run the code from before. Recall that we imported packages, defined a function to generate data, and set some parameters for the forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from proglearn.forest import UncertaintyForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, d, var): \n",
    "    '''\n",
    "    Parameters\n",
    "    ---\n",
    "    n : int\n",
    "        The number of data to be generated\n",
    "    d : int\n",
    "        The number of features to generate for each data point\n",
    "    var : double\n",
    "        The variance in the data\n",
    "    '''\n",
    "    # create the mean matrix for the data (here it's just a mean of 1)\n",
    "    means = [np.ones(d) * -1, np.ones(d)] \n",
    "    \n",
    "    # create the data with the given parameters (variance)\n",
    "    X = np.concatenate([np.random.multivariate_normal(mean, var * np.eye(len(mean)), \n",
    "                                                 size=int(n / 2)) for mean in means]) \n",
    "    \n",
    "    # create the labels for the data\n",
    "    y = np.concatenate([np.ones(int(n / 2)) * mean_idx for mean_idx in range(len(means))])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Params.\n",
    "n_train = 50\n",
    "n_test = 10000\n",
    "d = 100\n",
    "var = 0.25\n",
    "num_trials = 10\n",
    "n_estimators = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We've done a lot. Can we just run it now? Yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Creating & Training our UncertaintyForest \n",
    "First, generate our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(n_train+n_test, d, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, split that data into training and testing data. We don't want to accidently train on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:n_train] # Takes the first n_train number of data points and saves as X_train\n",
    "y_train = y[0:n_train] # same as above for the labels\n",
    "X_test = X[n_train:] # Takes the remainder of the data (n_test data points) and saves as X_test\n",
    "y_test = y[n_train:] # same as above for the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create our forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "UF = UncertaintyForest(n_estimators = n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then fit our learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<proglearn.forest.UncertaintyForest at 0x158283780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we're done. Exciting right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Producing a Metric of Accuracy for Our Learner\n",
    "We've now created our learner and trained it. But to actually show if what we did is effective at predicting the class labels of the data, we'll create some test data (with the same distribution as the train data) and see if we classify it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = generate_data(n_test, d, var) # creates the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = UF.predict(X_test) # predict the class labels of the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the learner's accuracy, we'll now compare the predictions with the actual test data labels. We'll find the number correct and divide by the number of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(predictions == y_test)/n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let's take a look at our accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta-da. That's an uncertainty forest at work. While that accuracy doesn't seem great, it is to-be-expected here since the data was randomly generated. In the next tutorial, the actual importance of uncertainty forests will be discussed.\n",
    "## What's next? \n",
    "Next, we'll produce some plots to demonstrate the power of uncertainty forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done with part 4 of the tutorial!\n",
    "\n",
    "## Move on to part 5 (called \"UncertaintyForest_Tutorial_5-Analyzing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
